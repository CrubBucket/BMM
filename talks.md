# Distributions, expectation, likelihood

* [Wishart distribution: characteristics, properties, application in machine learning and data analysis](student_talks/week_1_wishart) (reporter: Konstantin Yakovlev) 
* ~~Gumbel distribution (not Gumbel-softmax): characteristics, properties, application in machine learning and data analysis (reporter: *enter your name*)~~



# Bayesian inference
* ~~Linear model hyperparameter optimization: integrate vs. maximize [(link to the article)](https://bayes.wustl.edu/MacKay/alpha.pdf) (reporter: *enter your name*)~~
* The Counter-intuitive Non-informative Prior for the Bernoulli Family [(link to the article)](https://www.tandfonline.com/doi/pdf/10.1080/10691898.2004.11910734) (reporter: Skorik Sergey)

# Minimum description length
* BIC [article](https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full) (reporter: Yakovlev Konstantin) **NOTE:** this topic is classic, you can use more references to BIC, if you wish.
* ~~Kolmogorov structure complexity [article](https://homepages.cwi.nl/~paulv/papers/structure.pdf)  (reporter: *enter your name*)~~

# Variational inference 1
* Bayesian probabilistic propagation [article](https://arxiv.org/abs/1502.05336) (reporter: Barabanshchikova Polina)
* ~~Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) (reporter: *enter your name*)~~
* ~~WBIC [article](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) (reporter: *enter your name*)~~

# Variational inference 2
* ~~SGD as a variational distribution [article](https://www.jmlr.org/papers/volume18/17-214/17-214.pdf) (reporter: Skorik Sergey)~~
* Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) (reporter: *enter your name*)

# Graphical models
* ~~S-VAE [aritlce](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_24.pdf), [slides](http://web.cs.ucla.edu/~yzsun/classes/2020Winter_CS249/Papers/Group7_SVAE.pdf) (reporter: *enter your name*)~~
* Message Passing Graph Neural Networks  [Tutorial 1](https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks-GNNs---VmlldzoyMDI2NTg2) [Tutorial 2](https://towardsdatascience.com/the-intuition-behind-graph-convolutions-and-message-passing-6dcd0ebf0063) [Tutorial 3](https://paperswithcode.com/method/mpnn)  (reporter: Barabanshchikova Polina)

# Generative vs Discriminative
* ~~Generative vs. Discriminative:  [article](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-CVPR-05.pdf) (reporter: *enter your name*)~~
* Your classifier is secretly an energy based model and you should treat it like one [article](https://arxiv.org/pdf/1912.03263.pdf) (reporter: Kovaleva Maria)

# Lab 1 discussion
* Gumbel distribution (not Gumbel-softmax): characteristics, properties, application in machine learning and data analysis (reporter: Skorik Sergey)
* ~~Linear model hyperparameter optimization: integrate vs. maximize [(link to the article)](https://bayes.wustl.edu/MacKay/alpha.pdf) (reporter: *enter your name*)~~
* ~~Kolmogorov structure complexity [article](https://homepages.cwi.nl/~paulv/papers/structure.pdf)  (reporter: *enter your name*)~~
* ~~Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) (reporter: *enter your name*)~~
* ~~WBIC [article](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) (reporter: *enter your name*)~~
* ~~S-VAE [aritlce](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_24.pdf), [slides](http://web.cs.ucla.edu/~yzsun/classes/2020Winter_CS249/Papers/Group7_SVAE.pdf) (reporter: *enter your name*)~~
* ~~Generative vs. Discriminative:  [article](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-CVPR-05.pdf) (reporter: *enter your name*)~~

# Data generation
* ~~Out-of-distribution detection via probability model entropy  [article](https://arxiv.org/pdf/1703.04977.pdf), [workshop](https://www.youtube.com/watch?v=N-p_qSLzoAI) (reporter: *enter your name*)~~
* Diffusion models [article](https://arxiv.org/pdf/2208.11970.pdf) (reporter: Gregory Polyakov)

# Hierarchical models
* Ladder VAE [article](https://proceedings.neurips.cc/paper/2016/file/6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf) (reporter: Maria Kovaleva)
* On the information bottleneck theory of deep learning [article](https://openreview.net/pdf?id=ry_WPG-A-) (reporter: Polina Barabanshchikova)

# Lab 2 discussion
~~* Out-of-distribution detection via probability model entropy  [article](https://arxiv.org/pdf/1703.04977.pdf), [workshop](https://www.youtube.com/watch?v=N-p_qSLzoAI) (reporter: *enter your name*)~~

## Model ensembles, Mixture of experts
* ~~Neural-Tangent-Kernel-ensembles [article](https://arxiv.org/pdf/2202.12297.pdf) (reporter: *enter your name*)~~
* Prior Networks [article](https://arxiv.org/abs/1802.10501)  (reporter: Ksenofontov Gregory)

## Structure
*  Neural Architecture Search without Training [article](https://arxiv.org/abs/2006.04647)  (reporter:  *enter your name*)
*  Deep neural decision trees [article](https://arxiv.org/pdf/1806.06988.pdf)  (reporter:  Pyatkin Stanislav)

# Random processes and genetics for model generation
* ~~[AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](http://proceedings.mlr.press/v119/real20a/real20a.pdf) (reporter:  *enter your name*)~~
* ~~[Proving the Lottery Ticket Hypothesis: Pruning is All You Need](http://proceedings.mlr.press/v119/malach20a/malach20a.pdf) (reporter:  *enter your name*)~~

# Hyperparameter optimization
~~* [Hypernetwork-based gradient optimization](https://arxiv.org/abs/1802.09419) (reporter:  *enter your name*)~~
* [Dual form of SGD via Attention](https://proceedings.mlr.press/v162/irie22a.html) (reporter:  Skorik Sergey)

# SMBO
* [Learning Pareto front with hypernetworks](https://arxiv.org/pdf/2010.04104.pdf) (reporter:  Barabanshchikova Polina)
* [Efficient hyperparameter optimization for deep learning algorithms using deterministic rbf surrogates](https://ojs.aaai.org/index.php/AAAI/article/view/10647/10506)  (reporter:  *Yakovlev Konstantin*)


# Meta-optimization
* ~~[Generalized Inner Loop Meta-Learning](https://arxiv.org/pdf/1910.01727.pdf) (reporter:  *enter your name*)~~
* ~~[HOW TO TRAIN YOUR MAML](https://arxiv.org/pdf/1810.09502.pdf)  (reporter:  *enter your name*)~~

# Lab 4 discussion
* ~~[AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](http://proceedings.mlr.press/v119/real20a/real20a.pdf) (reporter:  *enter your name*)~~
* ~~[Proving the Lottery Ticket Hypothesis: Pruning is All You Need](http://proceedings.mlr.press/v119/malach20a/malach20a.pdf) (reporter:  *enter your name*)~~
* ~~[Hypernetwork-based gradient optimization](https://arxiv.org/abs/1802.09419) (reporter:  *enter your name*)~~
* ~~[Generalized Inner Loop Meta-Learning](https://arxiv.org/pdf/1910.01727.pdf) (reporter:  *enter your name*)~~
* ~~[HOW TO TRAIN YOUR MAML](https://arxiv.org/pdf/1810.09502.pdf)  (reporter:  *enter your name*)~~

# Sampling and prior selection
~~* [SWAG: A simple baseline for bayesian uncertainty in deep learning](https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf)  (reporter:  *enter your name*)~~
~~* [Neural Architecture Search Using Deep Neural Networks and Monte Carlo Tree Search](https://ojs.aaai.org/index.php/AAAI/article/view/6554) (reporter:  *enter your name*)~~

# Probabilistic metric spaces
* [f -Divergence Variational Inference](https://proceedings.neurips.cc/paper/2020/file/c928d86ff00aeb89a39bd4a80e652a38-Paper.pdf)   (reporter:  Skorik Sergey)
* [Task-Adaptive Neural Network Search with Meta-Contrastive Learning](https://arxiv.org/pdf/2103.01495.pdf)  (reporter:  *enter your name*)

# Lab 5 discussion
* ~~[SWAG: A simple baseline for bayesian uncertainty in deep learning](https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf)  (reporter:  *enter your name*)~~
* ~~[Neural Architecture Search Using Deep Neural Networks and Monte Carlo Tree Search](https://ojs.aaai.org/index.php/AAAI/article/view/6554) (reporter:  *enter your name*)~~
* ~~[Task-Adaptive Neural Network Search with Meta-Contrastive Learning](https://arxiv.org/pdf/2103.01495.pdf)  (reporter:  *enter your name*)~~

# Transfer learning
* [Learning to select data for transfer learning with bayesian optimization](https://arxiv.org/pdf/0907.1815.pdf) (reporter:  *enter your name*)
* [Knowledge transfer via dense cross-layer mutual-distillation](https://arxiv.org/pdf/2008.07816)  (reporter:  *enter your name*)
